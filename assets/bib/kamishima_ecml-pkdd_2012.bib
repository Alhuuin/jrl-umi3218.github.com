@InProceedings{kamishima:ecml-pkdd:2012,
  author    = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
  title     = {Fairness-aware Classifier with Prejudice Remover Regularize},
  booktitle = {The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases},
  year      = {2012},
  pages     = {35--50},
  address   = {Bristol, UK},
  month     = {September 24-September 28},
  url       = {https://link.springer.com/content/pdf/10.1007\%2F978-3-642-33486-3\_3.pdf},
  keywords  = {fairness, discrimination, logistic regression, classification, social responsibility, information theory},
  doi       = {DOI:10.1007/978-3-642-33486-3\_3},
  abstract  = {With the spread of data mining technologies and the accumulation of social data, such technologies and data are being used for determinations that seriously affect individualsâ€™ lives. For example, credit scoring is frequently determined based on the records of past credit data together with statistical prediction techniques. Needless to say, such determinations must be nondiscriminatory and fair in sensitive features, such as race, gender, religion, and so on. Several researchers have recently begun to attempt the development of analysis techniques that are aware of social fairness or discrimination. They have shown that simply avoiding the use of sensitive features is insufficient for eliminating biases in determinations, due to the indirect influence of sensitive information. In this paper, we first discuss three causes of unfairness in machine learning. We then propose a regularization approach that is applicable to any prediction algorithm with probabilistic discriminative models. We further apply this approach to logistic regression and empirically show its effectiveness and efficiency.}
}